@inproceedings{Fonseca1993,
author = {Fonseca, C M and Fleming, P J},
booktitle = {Proceedings of the Fifth International Conference},
doi = {citeulike-article-id:2361311},
file = {:C$\backslash$:/Users/samue/Downloads/Papers/10.1.1.48.9077.pdf:pdf},
isbn = {1-55860-299-2},
issn = {14639076},
number = {July},
pages = {416--423},
title = {{Genetic Algorithms for Multiobjective Optimization: Formulation, Discussion and Generalization, Genetic Algorithms}},
year = {1993}
}
@article{Zhu2005,
abstract = {Extreme learning machine (ELM) [G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: a new learning scheme of feedforward neural networks, in: Proceedings of the International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25-29 July 2004], a novel learning algorithm much faster than the traditional gradient-based learning algorithms, was proposed recently for single-hidden-layer feedforward neural networks (SLFNs). However, ELM may need higher number of hidden neurons due to the random determination of the input weights and hidden biases. In this paper, a hybrid learning algorithm is proposed which uses the differential evolutionary algorithm to select the input weights and Moore-Penrose (MP) generalized inverse to analytically determine the output weights. Experimental results show that this approach is able to achieve good generalization performance with much more compact networks. {\textcopyright} 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
author = {Zhu, Qin Yu and Qin, A. K. and Suganthan, P. N. and Huang, Guang Bin},
doi = {10.1016/j.patcog.2005.03.028},
file = {:C$\backslash$:/Users/samue/Downloads/Papers/doi{\_}10.1016{\_}j.patcog.2005.03.028.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Differential evolution,Extreme learning machine,Minimum norm least square},
number = {10},
pages = {1759--1763},
title = {{Evolutionary extreme learning machine}},
volume = {38},
year = {2005}
}
@article{Goldberg1988,
abstract = {Regression testing is a necessary but expensive maintenance$\backslash$nactivity aimed at showing that code has not been adversely affected by$\backslash$nchanges. Regression test selection techniques reuse tests from an$\backslash$nexisting test suite to test a modified program. Many regression test$\backslash$nselection techniques have been proposed, however, it is difficult to$\backslash$ncompare and evaluate these techniques because they have different goals.$\backslash$nThis paper outlines the issues relevant to regression test selection$\backslash$ntechniques, and uses these issues as the basis for a framework within$\backslash$nwhich to evaluate the techniques. The paper illustrates the application$\backslash$nof the framework by using it to evaluate existing regression test$\backslash$nselection techniques. The evaluation reveals the strengths and$\backslash$nweaknesses of existing techniques, and highlights some problems that$\backslash$nfuture work in this area should address},
annote = {Genetic algorithms are a Machine Learning algorithm that uses a natural metaphor. In comparison to neuronal algorithms, genetics are better studied and sufficiently complex as well.
This issue studies classifier systems where the implicit parallelity of genetic algorithms has been made explicit.},
author = {Goldberg, David E. and Holland, John H.},
doi = {10.1023/A:1022602019183},
file = {:C$\backslash$:/Users/samue/Downloads/Papers/10.1023{\%}2FA{\_}1022602019183.pdf:pdf},
isbn = {0-19-854063-9},
issn = {15730565},
journal = {Machine Learning},
number = {2},
pages = {95--99},
pmid = {20835807},
title = {{Genetic Algorithms and Machine Learning}},
volume = {3},
year = {1988}
}
@article{Pang2002,
abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
archivePrefix = {arXiv},
arxivId = {cs/0205070},
author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
doi = {10.3115/1118693.1118704},
eprint = {0205070},
file = {:C$\backslash$:/Users/samue/Downloads/Papers/p79-pang.pdf:pdf},
isbn = {1932432515},
issn = {1554-0669},
journal = {Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - EMNLP '02},
number = {July},
pages = {79--86},
primaryClass = {cs},
title = {{Thumbs up? Sentiment Classification using Machine Learning Techniques}},
url = {http://dl.acm.org/citation.cfm?id=1118693.1118704{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1118693.1118704},
volume = {10},
year = {2002}
}
@article{Whittington2017,
abstract = {With our ability to record more neurons simultaneously, making sense of these data is a challenge. Functional connectivity is one popular way to study the relationship between multiple neural signals. Correlation-based methods are a set of currently well-used techniques for functional connectivity estimation. However, due to explaining away and unobserved common inputs (Stevenson et al., 2008), they produce spurious connections. The general linear model (GLM), which models spikes trains as Poisson processes (Okatan et al., 2005; Truccolo et al., 2005; Pillow et al., 2008), avoids these confounds. We develop here a new class of methods by using differential signals based on simulated intracellular voltage recordings. It is equivalent to a regularized AR(2) model. We also expand the method to simulated local field potential (LFP) recordings and calcium imaging. In all of our simulated data, the differential covariance-based methods achieved better or similar performance to the GLM method and required fewer data samples. This new class of methods provides alternative ways to analyze neural signals.},
annote = {Topic: Simple Hebbian model approximates neural network error backpropagation.

Two classes of biological supervised learning: 
stochastic neurons and synapses receiving a global feedback signal (via a neuromodulator (long-term neurotransmitter)) - often does not approximate backpropagation and scales poorly.
Explicitly approximates backpropagation.

Predictive coding framework uses additional nodes that encode the difference between the activity and the predicted activity on a certain level.

In Artifical Neural Networks, node correction is obtained by a global error estimate that can be computed via backpropagation. The errors could as well be included as separate nodes.},
archivePrefix = {arXiv},
arxivId = {1706.02451},
author = {Whittington, James C. R. and Bogacz, Rafal},
doi = {10.1162/NECO},
eprint = {1706.02451},
file = {:C$\backslash$:/Users/samue/Downloads/Papers/Error Backpropagation Algorithm.pdf:pdf},
isbn = {0899-7667},
issn = {1530888X},
pages = {1229--1262},
pmid = {25602775},
title = {{An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity}},
url = {http://arxiv.org/abs/1706.02451},
volume = {1262},
year = {2017}
}
