\documentclass[a4paper,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage[svgnames]{xcolor}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath,amssymb,textcomp}
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{chemfig} % Independence sets
\usepackage[backend=bibtex,
style=numeric,
bibencoding=ascii
%style=alphabetic
%style=reading
]{biblatex}
\addbibresource{C:/Users/samue/Documents/Mathematik-BA_Mathe.bib}


\geometry{
left=20mm,right=20mm,%
bindingoffset=0mm, top=20mm,bottom=15mm}

\hypersetup{
    bookmarksnumbered=true,
    bookmarksopen=false,
    bookmarksopenlevel=1,
    colorlinks=true,
    linkcolor=blue,
    urlcolor=DarkBlue,
    citecolor=DarkRed
}

\newcommand{\figur}[5][0]{
		\begin{figure}[H] \centering \em %H / h!
			\includegraphics[width=#5\textwidth, angle=#1]{#2}
			\caption{#3}\label{fig:#4}
		\end{figure}
}

\linespread{1.3}

\newcommand{\linia}{\rule{\linewidth}{0.5pt}}

% my own titles

%%%

\newcommand{\const}{\varsigma}
\newcommand{\var}{\chi}

% custom footers and headers
\usepackage{fancyhdr,lastpage}
\pagestyle{headings}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\usepackage[thref,thmmarks,amsmath,framed,hyperref]{ntheorem}
\hbadness=10001
\hfuzz=1000pt
\makeatletter\let\Author\@author\makeatother
\author{Samuel Lippl}
\title{Overview: Backpropagation and Predictive Coding}
\begin{document}
\renewcommand{\abstractname}{\vspace{-\baselineskip}}
\theoremstyle{change} %Nummer vor Theoremtitel
\theoremheaderfont{\normalfont\scshape}
\theoremseparator{.}
\theorembodyfont{\normalfont}
\newtheorem{Def}{Definition}[section]
\newtheorem{The}[Def]{Theorem}
\newtheorem{Lem}[Def]{Lemma}
\newtheorem{Pro}[Def]{Proposition}
\newtheorem{Kor}[Def]{Korollar}
\newtheorem{Bem}[Def]{Remark}
\newtheorem{Not}[Def]{Notation}
\newtheorem{Bei}[Def]{Example}
\newtheorem{Ax}[Def]{Axiom}
\newtheorem{Con}[Def]{Condition}
\newtheorem{Hyp}[Def]{Hypothesis}
\newtheorem{OP}{Open Problem}
\theoremseparator{}
\newtheorem{Abs}[Def]{}
\theoremstyle{nonumberplain}
\theoremheaderfont{\normalfont\itshape}
\theoremsymbol{$\square$}
\theoremseparator{.}
\newtheorem{Bew}{Proof}
%Lange Beweise enthalten oft Lemmata oder einzelne Teilbeweise kleinere Behauptungen. Diese werden wir Hilfslemmata nennen. Der Hauptunterschied zwischen Lemmata und Hilfslemmata ist, das Hilfslemmata spezifisch zum Theorem gehören und griechisch numeriert sind, während Lemmata unabhängig von diesem sind.
\theoremstyle{change}
\theoremindent1cm
\theoremseparator{.}
\theoremsymbol{}
\theoremheaderfont{\normalfont\scshape}
\newtheorem{BLem}[Def]{Lemma}
\theoremstyle{plain}
\theoremindent1cm
\theoremseparator{.}
\theoremnumbering{greek}
\theoremheaderfont{\normalfont\itshape}
\newtheorem{HLem}{Sublemma}[Def]
\newtheorem{BAbs}[HLem]{}
\theoremstyle{nonumberplain}
\theoremindent1cm
\theoremseparator{.}
\theoremheaderfont{\normalfont\itshape}
\theoremsymbol{$\triangle$}
\newtheorem{BBew}{Proof}
\maketitle
\tableofcontents
\textbf{Open Problems: Overview}\\
\listtheorems{OP}
\section{Backpropagation}
\begin{Abs}
We consider an additive feed-forward neural network in layered structure $\theta\in\mathbb{R}_{c,v}^{\mathcal{V}\times\mathcal{V}}$.
\begin{OP}[Layered structure]
Construction of layers and the corresponding order on $\mathcal{V}$.
\end{OP}
The step function is therefore
\begin{equation}
T_{\theta}^s=Wf\vartriangleright s
\end{equation}
and the canonical network function is
\begin{equation}
\mathbb{R}^{\mathcal{V}^{(0)}}\ni i\mapsto N_{\theta}^i=Wf\vartriangleright i\downharpoonleft\in\mathbb{R}^{\mathcal{V}}
\end{equation}
where we denote the corresponding output function by
\begin{equation}
O_{\theta}^i\in\mathbb{R}^{\mathcal{V}^{(L)}}
\end{equation}
We will often leave $\theta$ and $i$ implicit where they are clear from context.
\end{Abs}
\begin{Abs}[Backpropagation Objectve Function]
The backpropagation algorithm is concerned with optimizing the prediction that is generated by a neural network with respect to some loss function. We will first consider the quadratic loss function between a prediction $O$ generated from some input and the real output $o\in\mathbb{R}^{\mathcal{V}^{(L)}}$\footnote{
The given framework is easily extendable to a training set with more than one observation.}
\begin{equation}
E_{\theta}^{i;o}:=E(O_{\theta}^i,o):=\frac12(O_{\theta}^i-o)^T(O_{\theta}^i-o)
\end{equation}
$\theta$ is therefore modified to reduce $E$ which we do by gradient descent:
\begin{equation}
\theta\mapsto\theta+\alpha\frac{\partial E}{\partial\theta}
\end{equation}
As the loss function is generally one-dimensional, we can consider $\frac{\partial E}{\partial \theta}$ to be analogous to $\theta$.
\end{Abs}
\begin{The}[Backpropagation Identity]\label{backprop-the}
Defining
\begin{equation}\label{deltadiff}
\delta_{\text{diff}}:=O_{\theta}^i-o
\end{equation}
and
\begin{equation}\label{delta}
\delta:=f'(N)\circ\left(\theta\delta_{\text{diff}}\upharpoonleft\right)
\end{equation}
we obtain the following identity:
\begin{equation}\label{backprop-identity}
\frac{\partial E}{\partial \theta}=\delta f(N)^T
\end{equation}
\end{The}
\begin{Bew}
(\ref{backprop-identity}) follows from a repeated application of the chain rule. We first prove that $\delta=\frac{\partial E}{\partial N}$. We start by observing that
\begin{equation}
\frac{\partial E}{\partial N(\mathcal{V}^{(L)}}=\frac{\partial E}{\partial O}=\delta_{\text{diff}}
\end{equation}
For $v\notin\mathcal{V}^{(L)}$, we can write recursively (as we have a feed-forward network)
\begin{equation}
\frac{\partial E}{\partial N(v)}=\frac{\partial E}{\partial N(v\to)}\frac{\partial N(v\to)}{\partial N(v)}=\frac{\partial E}{\partial N(v\to)}\theta_{v\to}(v\to)\cdot f'(N(v\to))
\end{equation}
As all other entries in $\theta_{v\to}$ are $0_{\const}$, this corresponds to the recursive definition in (\ref{delta})\footnote{The multiplication by $f'(N)$ does not have to be part of the recursive definition.}.\\
As
\begin{equation}
\frac{\partial N(w)}{\partial\theta_{v\to w}}=f(w)
\end{equation}
and therefore
\begin{equation}
\frac{\partial E}{\partial\theta_{v\to w}}=\delta(v)f(w)
\end{equation}
we obtain (\ref{backprop-identity}) which proves the theorem.
\end{Bew}
\begin{Bem}
Backpropagation is only defined on feedforward networks, where the proof of theorem \ref{backprop-the} demonstrates the point at which a recurrent network would fail the algorithm. While we can also formulate the backpropagation algorithm for non-additive neural networks, we do not win much by that. It is the particular additive structure that allows the concise recursive notation.
\end{Bem}
\section{Predictive Coding}
I will present the predictive coding model according to \cite{Whittington2017} in a generalized framework.
\begin{Abs}[Generative Model]
We first consider a so-called \emph{generative model} $\xi\in\mathbb{R}^{\mathcal{V}_{\text{gen}}\times\mathcal{V}_{\text{gen}}}$ as a neural network which is a stochastic additive feed-forward network that is developed depending on input states $s(\mathcal{V}^{(0)}_{\text{gen}})$ by
\begin{equation}
s(v)=\mu_v\left(s(\to v)\right)+\epsilon_v\qquad \epsilon\sim\mathcal{N}(0,\Sigma)
\end{equation}
where $\Sigma\in\mathbb{R}^{\mathcal{V}_{\text{gen}}}$ is a positive definite covariance matrix and the expected value is
\begin{equation}
\mu_v\left(s(\to v)\right)=\xi_{\to v}^Ts(\to v)
\end{equation}
\end{Abs}
\begin{Bem}
$\Sigma$ is often a diagonal matrix or only contains correlations across the same layers.
\end{Bem}
\begin{Abs}[Objective Function of Predictive Coding]
While backpropagation is focused on optimizing the prediction, predictive coding attempts to find the most likely state of the entire generative model. As is often case, we consider the log-likelihood and our objective function is
\begin{equation}
F_{\xi,\Sigma}(s):=\ln P\left(s\left(\mathcal{V}\setminus\mathcal{V}^{(0)}_{\text{gen}}\right)|s\left(\mathcal{V}^{(0)}_{\text{gen}}\right)\right)
\end{equation}
which we obtain in proposition\ref{pro-predcode-obj}.
\end{Abs}
\begin{Pro}\label{pro-predcode-obj}
Optimizing $F$ corresponds to optimizing
\begin{equation}\label{eq-predcode-obj2}
-\frac{1}{2}\left(\ln\left(\det\Sigma\right)+\epsilon^T\Sigma^{-1}\epsilon\right)
\end{equation}
where we have only ommitted a constant term. We therefore denote (\ref{eq-predcode-obj2}) by $F$.
\end{Pro}
\begin{Bew}
As $\epsilon\in\mathbb{R}^{\mathcal{V}\setminus\mathcal{V}^{(0)}}$, 
\begin{equation}\label{propo1}
F_{\xi,\Sigma}(s)=\ln p\left(\epsilon|0,\Sigma\right)
\end{equation}
where $p$ is the multivariate normal distribution with expectation $0$ and covariance $\Sigma$:
\begin{equation}\label{propo2}
p(\epsilon|0,\Sigma)=\frac{1}{\sqrt{(2\pi)^{|\mathcal{V}|}\det\Sigma}}\exp\left(-\frac12\epsilon^T\Sigma^{-1}\epsilon\right)
\end{equation}
Note that $F$ in the form of (\ref{propo1}) still depends on $s$ as 
\begin{equation}
\epsilon = s-\xi^Tf(s)
\end{equation}
where we omit all $v\in\mathcal{V}^{(L)}$ as their error does not behave stochastically but is trivially zero.\\
Logarithmizing (\ref{propo2}) yields
\begin{equation}
\ln p=-\ln\left(\sqrt{(2\pi)^{|\mathcal{V}|}}\right)-\ln\left(\sqrt{\det\Sigma}\right)-\frac12\epsilon^T\Sigma^{-1}\epsilon
\end{equation}
where we omit the constant term which, together with some basic transformations yields (\ref{eq-predcode-obj2}).
\end{Bew}
\begin{Abs}[Inference]\label{inference}
Clearly, if we are are given $s(\mathcal{V}^{(0)})$, the $F$-optimal prediction of all other states would be 
\begin{equation}
\theta^Ts(\mathcal{V}^{(0)})\downharpoonleft
\end{equation}
However, we generally cannot observe $\mathcal{V}^{(0)}$ which corresponds to a cause that may be an abstract concept such as a face and therefore even constructed by our brain. In the general framework, there is a subset $\mathcal{I}\subseteq\mathcal{V}$ of which we know the value and have to infer the state of the remaining network $\xi$ by optimizing $F$ over the PUs $\mathcal{V}\setminus\mathcal{I}=:\mathcal{H}$.
\end{Abs}
\begin{The}[Predictive Coding Model I: Prediction]\label{the-pcm1}
The neural network with the step function on $s\in\mathbb{R}^{\mathcal{V}\times\mathcal{E}}$
\begin{align}
\label{pcm1-N}T^{(s,\varepsilon)}_{\xi}(\mathcal{V})&=s-\varepsilon+\text{diag}\left(f'(s)\right)\xi\varepsilon\\
\label{pcm1-eps}T^{(s,\varepsilon)}_{\xi}(\mathcal{E})&=\Sigma^{-1}\left(s-\xi^Tf(s)\right)\end{align}
performs gradient descent on $F$. By leaving $\mathcal{I}$ constant, we may perform inference according to \ref{inference}.
\end{The}
\begin{Bew}
We observe that
\begin{equation}\label{pcm1-eq1}
\frac{\partial F}{\partial N}=\frac{\partial F}{\partial\varepsilon}\frac{\partial\varepsilon}{\partial N}
\end{equation}
and, as
\[\varepsilon=\Sigma^{-1}\epsilon\]
we can write
\begin{equation}\label{pcm1-eq2}
F=-\frac12\varepsilon^T\Sigma\varepsilon
\end{equation}
Therefore
\begin{equation}\label{pcm1-eq3}
\frac{\partial F}{\partial\varepsilon}=-\varepsilon^T\Sigma
\end{equation}
and
\begin{equation}\label{pcm1-eq4}
\frac{\partial\varepsilon}{\partial N}=\Sigma^{-1}\left(I-\xi^T\text{diag}(f'(N))\right)
\end{equation}
where $I\in\mathbb{R}^{\mathcal{V}\times\mathcal{V}}$ is the identity matrix and $J=\text{diag}(f'(N))$ is the diagonal matrix where $J_{v\to v}=f'(N(v))$. We can now put together (\ref{pcm1-eq1},\ref{pcm1-eq3},\ref{pcm1-eq4}) to obtain (\ref{pcm1-N})
\end{Bew}
\begin{Abs}[Learnable $\Sigma$]
Theorem \ref{the-pcm1} provides an implicit definition of a neural network which is neither additive nor feed-forward.
\begin{OP}[Connection weights as learnable parameters]
I am still missing the section where I give a reason for the following.
\end{OP}
Additionally, we would like to treat $\Sigma$ as learnable parameters and therefore want to identify them with connection weights. The following definition provides a neural network with a weight matrix that contains all learnable parameters. It is important to note that while the network is not additive, it would be relatively easy to render the network additive by defining additional nodes for $f$ and $f'$. I do not see the point of such a definition as the network in theorem \ref{pcm2-the} is biologically plausible according to \cite{Whittington2017}.
\end{Abs}
\begin{Def}[Predictive Coding Model II: Predictive Network]\label{pcm2-def}
If we define error nodes for all PUs but the first layer of the generative model
\begin{equation}
\varepsilon:=\left\{\varepsilon_v|v\in\mathcal{V}_{\text{gen}}\setminus\mathcal{V}^{(0)}_{\text{gen}}\right\}
\end{equation}
the matrices
\begin{equation}
I_{\mathcal{V}}=\begin{pmatrix}
1_{\const}&&\\&\ddots&\\&&1_{\const}
\end{pmatrix}\in\mathbb{R}_{c,v}^{\mathcal{V}\times\mathcal{V}}, 
I_{\varepsilon}=\begin{pmatrix}
1_{\const}&&\\&\ddots&\\&&1_{\const}
\end{pmatrix}\in\mathbb{R}_{c,v}^{\varepsilon\times\varepsilon}
\end{equation}
the Processing Units
\begin{equation}
\mathcal{V}=\mathcal{V}_{\text{gen}}\cup\varepsilon
\end{equation}
and $\xi^{\varepsilon}\in\mathbb{R}^{\varepsilon\times\mathcal{V}_{\text{gen}}}$ (resp. $I_{\mathcal{V}}'\in\mathbb{R}^{\varepsilon\times\mathcal{V}_{\text{gen}}}$) as the weight matrix $\xi$ (resp. $I_{\mathcal{V}}$) without the rows of the input layer $\mathcal{V}^{(0)}$, the \emph{predictive coding network} is defined by
\begin{equation}
\theta=\begin{pmatrix}
I_{\mathcal{V}}&-\xi^T+I_{\mathcal{V}}'^T\\
-I_{\mathcal{V}}'+\xi&(I_{\varepsilon}-\Sigma)^T
\end{pmatrix}\in\mathbb{R}^{\mathcal{V}\times\mathcal{V}}
\end{equation}
together with the unit functions
\begin{align}
\label{pcm2-v}g_v(s|\theta_{\to v})&:=\theta_{\to v}^T(\mathcal{V})s(\mathcal{V}) + \theta_{\to v}^T(\varepsilon)s(\varepsilon)f'(N(v))\\
\label{pcm2-eps}g_{\varepsilon_v}(s|\theta_{\to\varepsilon_v})&:=\theta_{\to\varepsilon_v}^T(\mathcal{V})f(s(\mathcal{V}))-f(s(v))+s(v)+\theta_{\to\varepsilon_v}^T(\varepsilon)s(\varepsilon)
\end{align}
\end{Def}
\begin{Bem}\label{pcm2-rem}
By expressing (\ref{pcm2-v},\ref{pcm2-eps}) by $\xi$ and $\Sigma$ instead of $\theta$, their definition and the similarity to (\ref{pcm1-N},\ref{pcm1-eps}) becomes clear:
\begin{align}
\label{pcm2-v'}g_v(s|\theta_{\to v})&=s(v)-s(\varepsilon)+f'(s(v))\xi_{v\to}^{\varepsilon}s(\varepsilon)\\
\label{pcm2-eps'}g_v(s|\theta_{\to \varepsilon_v})&=s(v)-\left(\xi_{\to v}^{\varepsilon}\right)^Ts(\mathcal{V})+(I_{\varepsilon}-\Sigma)s(\varepsilon)
\end{align}
\end{Bem}
\begin{The}\label{pcm2-the}
The predictive coding network has the local maxima of $F$ as fixed states.
\end{The}
\begin{Bew}
It is sufficient to prove that the fixed states of $\theta$ are identical to the fixed states of the step function defined in (\ref{pcm1-N},\ref{pcm1-eps}). We use the equations from remark \ref{pcm2-rem} where (\ref{pcm2-v'}) implies (\ref{pcm1-N}). For (\ref{pcm2-eps'}), we observe that for some fixed state $t$,
\[I_{\varepsilon}t(\varepsilon)=t(\varepsilon)=t(v)-\left(\xi_{\to v}^{\varepsilon}\right)^Tf(t(\mathcal{V}))+(I_{\varepsilon}-\Sigma)t(\varepsilon)\]
which is, again, equivalent to the fixed state equation corresponding to \ref{pcm1-eps}.
\end{Bew}
\begin{Abs}[Predictive Coding Model III: Learning Perspectives]
Theorem \ref{pcm2-the} can be taken as evidence that the predictive coding network is a reasonable implementation of the generative model in the brain. We will now turn towards the question how such a network may learn its parameters. With respect to this goal, we have two options: we may consider learning $\xi$ and $\Sigma$ or we may consider learning $\theta$. While every learning rule on $\xi$ and $\Sigma$ can be transformed into a learning rule on $\theta$ this transformation does not preserve the Hebbian property because of the two applications of $\xi$, a problem that has been coined \emph{weight symmetry} (see e. g. \cite[][1254]{Whittington2017}. On the other hand, not every learning rule on $\theta$ can be transformed into a learning rule on $\xi$ and $\Sigma$ but if there exists such a transformation, it preserves the Hebbian property.
\end{Abs}
\begin{Abs}[Predictive Coding Model IV: Learning $\xi$ and $\Sigma$]
In this framework, we begin by treating some fixed state of the network as a constant value and try to improve $\xi$ and $\Sigma$, again by gradient descent. This corresponds to the approach in \cite{Whittington2017} and is based on the expectation maximization algorithm \cite{Dempster1977} as presented in \cite{Friston2005}. In this framework, prediction corresponds to the estimation of the expected value of all units and therefore the states of the generative model if we fix the states of certain units. On the other hand, learning $\xi$ and $\Sigma$ corresponds to changes in the generative model the brain builds to describe its environment (see \cite{Friston2005}). 
\end{Abs}
\begin{The}[Learning $\xi$]\label{pcm4-xi-the}
If 
\begin{equation}\label{pcm4-xi-the-not}
\frac{\partial F}{\partial\xi^{\varepsilon}}\equiv\left(\frac{\partial F}{\partial\xi_{v\to w}}\right)_{(v,w)\in\mathcal{V}\times(\mathcal{V}\setminus\mathcal{V}^{(0)})}
\end{equation}
we obtain
\begin{equation}\label{pcm4-xi-the-eq}
\frac{\partial F}{\partial\xi^{\varepsilon}}=N(\varepsilon) f(N(\mathcal{V}))^T
\end{equation}
\end{The}
\begin{Bew}
\begin{align*}
\frac{\partial F}{\partial\xi_{v\to w}}&=-\varepsilon^T\Sigma\frac{\partial\varepsilon}{\partial\xi_{v\to w}}\\
\frac{\partial\varepsilon}{\partial\xi_{v\to w}}&=-\Sigma^{-1}\frac{\partial}{\partial\xi_{v\to w}}\xi^Tf(N)
\end{align*}
As $\xi^Tf(N)$ contains $\xi_{v\to w}$ exactly once, we have a vector with one non-zero entry. As $\xi_{v\to w}$ is in the $w$-row of $\xi^T$, it is in the $w$-row of $\xi^Tf(N)$. Evidently, the value in the $w$-row is $f(N(v))$ and therefore
\begin{equation}
\frac{\partial F}{\partial\xi_{v\to w}}=\varepsilon_wf(N(v))
\end{equation}
which, in the notation of (\ref{pcm4-xi-the-not}), yields (\ref{pcm4-xi-the-eq}).
\end{Bew}
\begin{Abs}[Hebbian Learning of $\xi$]
We note that $\xi$ can be learned in a Hebbian way that can be directly motivated from gradient ascent. We also note that the formula resembles the formula for backpropagation but is generally not identical because the network functions may yield different results.
\end{Abs}
\begin{Pro}[Learning $\Sigma$ I: Gradient Ascent]\label{pcm4-sigma1-pro}
If
\begin{equation}\label{pcm4-sigma1-not}
\frac{\partial F}{\partial\Sigma}\equiv\left(\frac{\partial F}{\partial\Sigma_{v\to w}}\right)_{(v,w)\in(\mathcal{V}\setminus\mathcal{V}^{(0)})\times(\mathcal{V}\setminus\mathcal{V}^{(0)})}
\end{equation}
we obtain
\begin{equation}\label{pcm4-sigma1-eq}
\frac{\partial F}{\partial\Sigma}=\frac12\Sigma^{-2}\circ(\epsilon\epsilon^T-\Sigma)
\end{equation}
\end{Pro}
\begin{Bew} Let $v,w\in\mathcal{V}\setminus\mathcal{V}^{(0)}$.
\begin{equation}\label{pcm4-sigma1-1}
\frac{\partial F}{\partial\Sigma_{v\to w}}=-\frac12\left(\frac{\partial}{\partial\Sigma_{v\to w}}\ln\det\Sigma+\frac{\partial}{\partial\Sigma_{v\to w}}\epsilon^T\Sigma^{-1}\epsilon\right)
\end{equation}
We obtain 
\begin{equation}\label{logdet}
\frac{\partial}{\partial\Sigma_{v\to w}}\ln\det\Sigma=\Sigma^{-1}_{w\to v}=\Sigma^{-1}_{v\to w}
\end{equation}
\begin{OP}[Justification of (\ref{logdet})]
Reconstruct the \hyperlink{https://math.stackexchange.com/questions/38701/how-to-calculate-the-gradient-of-log-det-matrix-inverse}{solution}.
\end{OP}
On the other hand, 
\begin{equation}\label{pcm4-sigma1-2}
\frac{\partial\epsilon^T\Sigma^{-1}\epsilon}{\partial\Sigma^{-1}}=\epsilon\epsilon^T
\end{equation}
togther with
\begin{equation}\label{pcm4-sigma1-3}
\frac{\partial\Sigma^{-1}}{\partial\Sigma}=-\Sigma^{-2}
\end{equation}
yields
\begin{equation}\label{pcm4-sigma1-5}
\frac{\partial}{\partial\Sigma_{v\to w}}\epsilon^T\Sigma^{-1}\epsilon=\frac{\partial}{\partial\Sigma_{v\to w}^{-1}}\epsilon^T\Sigma^{-1}\epsilon\frac{\partial\Sigma_{v\to w}^{-1}}{\partial\Sigma_{v\to w}}=-\varepsilon_v\varepsilon_w\left(\Sigma^{-2}\right)_{v\to w}
\end{equation}
Combining (\ref{logdet},\ref{pcm4-sigma1-5}) yields (\ref{pcm4-sigma1-eq}).
\end{Bew}
\begin{Abs}[Learning $\Sigma$ I]
Gradient ascent is clearly not Hebbian if $\Sigma$ is not diagonal, that is, if the error terms are correlated. Note that, we cannot obtain $\epsilon$ either. However, the direction of the gradient is given by $\epsilon\epsilon^T-\Sigma$ and at least does not make computation of the gradient necessary. \cite{Bogacz2017} proposes a solution for a less general situation.
\begin{OP}[Hebbian $\Sigma$]
How can we learn $\Sigma$ in a Hebbian way without adding more PUs than necessary?
\end{OP}
\end{Abs}
\printbibliography
\end{document}